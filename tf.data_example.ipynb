{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YouTokenToMe tokenizer\n",
    "https://github.com/vkcom/YouTokenToMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting youtokentome\n",
      "  Downloading youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 345 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Click>=7.0\n",
      "  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: Click, youtokentome\n",
      "Successfully installed Click-7.1.1 youtokentome-1.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install youtokentome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-ba15253e5fe6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-ba15253e5fe6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    BRYAK!!!!\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "BRYAK!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import subprocess\n",
    "import youtokentome as yttm\n",
    "import multiprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/1M.low.shuffled is file: True\n",
      "File size: 120.30 Mbytes\n"
     ]
    }
   ],
   "source": [
    "data_filepath = \"/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/1M.low.shuffled\"\n",
    "\n",
    "print(\"%s is file: %s\" % (data_filepath, os.path.isfile(data_filepath)))\n",
    "\n",
    "if os.path.isfile(data_filepath):\n",
    "    print(\"File size: %.2f Mbytes\" % (os.stat(data_filepath).st_size / 1024. / 1024.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test is file: True\n",
      "File size: 2845.08 Mbytes\n"
     ]
    }
   ],
   "source": [
    "test_data_filepath = \"/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test\"\n",
    "\n",
    "print(\"%s is file: %s\" % (test_data_filepath, os.path.isfile(test_data_filepath)))\n",
    "\n",
    "if os.path.isfile(test_data_filepath):\n",
    "    print(\"File size: %.2f Mbytes\" % (os.stat(test_data_filepath).st_size / 1024. / 1024.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 4000\n",
    "model_path = \"example.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trains BPE model and saves to file.  \n",
    "\n",
    "\n",
    "<code>youtokentome.BPE.train(data, model, vocab_size, coverage, n_threads=-1, pad_id=0, unk_id=1, bos_id=2, eos_id=3)</code>\n",
    "\n",
    "Args:  \n",
    "  \n",
    "**data**: string, path to file with training data  \n",
    "**model**: string, path to where the trained model will be saved  \n",
    "**vocab_size**: int, number of tokens in the final vocabulary  \n",
    "**coverage**: float, fraction of characters covered by the model. Must be in the range [0, 1]. A good value to use is about 0.9999.  \n",
    "**n_threads**: int, number of parallel threads used to run. If -1 is passed, then all available threads are going to be used. Note that the number of threads is limited by 8 (see benchmark).  \n",
    "**pad_id**: int, reserved id for padding  \n",
    "**unk_id**: int, reserved id for unknown symbols  \n",
    "**bos_id**: int, reserved id for begin of sentence token  \n",
    "**eos_id**: int, reserved id for end of sentence token  \n",
    "**Returns**: Class youtokentome.BPE with the loaded model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.42 s, sys: 756 ms, total: 10.2 s\n",
      "Wall time: 2.28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x7f566f209438>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Training model\n",
    "yttm.BPE.train(data=data_filepath, vocab_size=VOCAB_SIZE, model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 22s, sys: 12.2 s, total: 2min 34s\n",
      "Wall time: 30.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<youtokentome.youtokentome.BPE at 0x7f566d636208>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Training model\n",
    "yttm.BPE.train(data=test_data_filepath, vocab_size=VOCAB_SIZE, model=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading  \n",
    "\n",
    "\n",
    "<code>youtokentome.BPE(model, n_threads=-1)</code> \n",
    "\n",
    "Class constructor. Loads the trained model.  \n",
    "  \n",
    "**model**: string, path to the trained model  \n",
    "**n_threads**: int, number of parallel threads used to run. If equal to -1, then the maximum number of threads available will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 11.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loading model\n",
    "bpe = yttm.BPE(model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = open(test_data_filepath, \"rt\")\n",
    "test_texts = [test_file.readline().strip() for i in range(10)]\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two types of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107, 505, 3797, 1008, 1616, 3683, 129, 53, 884, 80, 3902, 276, 1915, 237, 362, 935, 104, 1615, 85, 912, 3021, 2350, 240, 193, 508, 57, 1949, 337, 80, 57, 973, 124, 57, 3013, 2571, 145, 2247, 107, 2939, 185, 1679, 85, 2133, 261, 104, 140, 2594, 86, 242, 299, 179, 57, 1739, 80, 53, 3013, 2877]\n",
      "[256, 219, 2282, 68, 17, 180, 1341, 2648, 2715, 1124, 686, 11, 2602, 1933, 3001, 277, 2013, 1049, 476, 85, 335]\n",
      "[250, 306, 250, 773, 501, 250, 773, 393, 57, 688, 155, 797, 1557, 85, 250, 375, 14, 538, 76, 553, 605, 10, 123, 124, 250, 589, 816, 219, 1882, 124, 392, 553, 1492, 103, 976, 267, 332, 1600, 362, 76, 1068, 85, 250, 294, 140, 3392, 2015]\n",
      "[397, 335, 421, 196, 3925, 438, 106, 1885, 72, 249, 3050, 1903, 2211, 2368, 249, 95, 3854, 79, 1131, 213, 85, 57, 973, 124, 53, 1780, 3925, 155, 2416, 72, 86, 140, 3161, 60, 135, 241, 207, 106, 1010, 368, 1938, 11, 107, 53, 1885, 76, 57, 1029, 2211, 2368]\n",
      "[57, 1419, 971, 11, 124, 740, 316, 17, 228, 89, 5, 147, 272, 7, 76, 699, 2999, 294, 2727, 86, 1168, 32, 96, 86, 740, 501, 1386, 115, 358, 1122, 62, 309, 7, 53]\n",
      "[1392, 57, 52, 198, 1644, 86, 1949, 57, 1313, 274, 314, 85, 1361, 86, 57, 1147, 1313]\n",
      "[3955, 99, 105, 3513, 148, 2871, 124, 2083, 1586, 3245, 72, 3384, 86, 95, 55, 76, 2630, 2239, 58, 987, 217, 81, 154, 3649, 221, 2853, 2537, 129, 58, 987, 217, 81, 332, 362, 716, 2709, 76, 1823, 218, 2963, 87, 225, 367, 107, 57, 1572, 969]\n",
      "[789, 2805, 332, 318, 68, 11, 79, 805, 85, 76, 3610, 295, 80, 209, 962, 5, 2619, 185, 352, 3633, 5, 952, 57, 2759, 85, 3824, 1708, 157, 266, 2679, 86, 786, 2805, 364, 729, 1870, 3077]\n",
      "[53, 1833, 2126, 1535, 3521, 1789, 543, 145, 1667, 2928, 467, 759, 76, 85, 345, 80, 57, 908, 2681, 241, 753, 2651, 124, 553, 2189, 115, 1443, 1520]\n",
      "[265, 628, 1208, 86, 2376, 57, 2527, 80, 57, 1020, 121, 1592, 145, 991, 1107, 212, 6]\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "print(*bpe.encode(test_texts, output_type=yttm.OutputType.ID), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁for', '▁cent', 'uries', '▁american', '▁democ', 'racy', '▁as', '▁a', '▁process', '▁of', '▁conflict', '▁res', 'olution', '▁has', '▁been', '▁based', '▁on', '▁give', '▁and', '▁take', '▁negot', 'iation', '▁comp', 'rom', 'ise', '▁the', '▁accept', 'ance', '▁of', '▁the', '▁fact', '▁that', '▁the', '▁majority', '▁rules', '▁with', '▁respect', '▁for', '▁minor', 'ity', '▁rights', '▁and', '▁above', '▁all', '▁on', '▁an', '▁agreement', '▁to', '▁ab', 'ide', '▁by', '▁the', '▁results', '▁of', '▁a', '▁majority', '▁vote']\n",
      "['▁they', '▁have', '▁created', '▁p', 'u', 'pp', 'ets', '▁veh', 'icles', '▁bas', 'ket', 's', '▁rob', 'ots', '▁jew', 'ell', 'ery', '▁build', 'ings', '▁and', '▁more']\n",
      "['▁you', '▁said', '▁you', '▁did', '▁what', '▁you', '▁did', '▁when', '▁the', '▁group', '▁was', '▁very', '▁young', '▁and', '▁you', '▁imp', 'l', 'ied', '▁in', '▁your', '▁mem', 'o', 'ir', '▁that', '▁you', '▁could', \"n't\", '▁have', '▁done', '▁that', '▁if', '▁your', '▁super', 'st', 'ruct', 'ure', '▁had', '▁already', '▁been', '▁in', '▁place', '▁and', '▁you', '▁were', '▁an', '▁older', '▁organisation']\n",
      "['▁no', '▁more', '▁than', '▁one', '▁appeal', '▁may', '▁be', '▁request', 'ed', '▁per', '▁cert', 'ification', '▁cy', 'cle', '▁per', '▁re', 'new', 'al', '▁applic', 'ant', '▁and', '▁the', '▁fact', '▁that', '▁a', '▁previous', '▁appeal', '▁was', '▁grant', 'ed', '▁to', '▁an', '▁interp', 're', 'ter', '▁will', '▁not', '▁be', '▁allow', 'able', '▁ground', 's', '▁for', '▁a', '▁request', '▁in', '▁the', '▁current', '▁cy', 'cle']\n",
      "['▁the', '▁pot', '▁still', 's', '▁that', '▁make', '▁ag', 'u', 'ard', 'ent', 'e', '▁de', '▁can', 'a', '▁in', '▁made', 'ira', '▁were', '▁brought', '▁to', '▁bra', 'z', 'il', '▁to', '▁make', '▁what', '▁today', '▁is', '▁also', '▁called', '▁c', 'ach', 'a', '▁a']\n",
      "['▁press', '▁the', '▁t', 'ab', '▁key', '▁to', '▁accept', '▁the', '▁field', '▁cont', 'ents', '▁and', '▁move', '▁to', '▁the', '▁next', '▁field']\n",
      "['▁richard', '▁g', 'id', '▁pow', 'ers', '▁wrote', '▁that', '▁ho', 'over', '▁install', 'ed', '▁felt', '▁to', '▁re', 'in', '▁in', '▁william', '▁c.', '▁s', 'ull', 'iv', 'an', \"'s\", '▁domestic', '▁sp', 'ying', '▁operations', '▁as', '▁s', 'ull', 'iv', 'an', '▁had', '▁been', '▁eng', 'aged', '▁in', '▁secret', '▁un', 'off', 'ic', 'ial', '▁work', '▁for', '▁the', '▁white', '▁house']\n",
      "['▁every', '▁hour', '▁had', '▁three', '▁p', 's', 'al', 'ms', '▁and', '▁in', '▁consequ', 'ence', '▁of', '▁this', '▁sever', 'e', '▁regular', 'ity', '▁there', '▁disapp', 'e', 'ared', '▁the', '▁deep', '▁and', '▁historical', '▁mot', 'ive', '▁which', '▁gave', '▁to', '▁each', '▁hour', '▁its', '▁own', '▁character', 'istics']\n",
      "['▁a', '▁challen', 'ging', '▁college', '▁prepar', 'atory', '▁program', '▁with', '▁significant', '▁achieve', 'ments', '▁both', '▁in', '▁and', '▁out', '▁of', '▁the', '▁class', 'room', '▁will', '▁help', '▁ensure', '▁that', '▁your', '▁application', '▁is', '▁compet', 'itive']\n",
      "['▁cl', 'ick', '▁here', '▁to', '▁watch', '▁the', '▁video', '▁of', '▁the', '▁test', 'im', 'ony', '▁with', '▁del', 'ah', 'un', 't']\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "print(*bpe.encode(test_texts, output_type=yttm.OutputType.SUBWORD), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpe.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpe.encode('i would like to know if darth vader was right', output_type=yttm.OutputType.SUBWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание TFRecords из очищенных текстовых корпусов, используя натренированный YouTokenToMe BPE токенайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 10\n",
    "num_cores = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Многопоточная реализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Следующая функция может быть использована чтобы преобразовать значение в тип совместимый с\n",
    "# с tf.Example.\n",
    "def _intlist_to_int64_feature(value):\n",
    "    \"\"\"Преобразует bool / enum / int / uint в int64_list.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_features = {\n",
    "    'X': tf.io.FixedLenFeature([SEQ_LEN], tf.int64),\n",
    "    \"y\": tf.io.FixedLenFeature([1], tf.int64)\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Разберите `tf.Example` proto используя вышеприведенный словарь.\n",
    "  return tf.io.parse_single_example(example_proto, keys_to_features)\n",
    "\n",
    "def _parse_batch_function(example_proto):\n",
    "  # Разберите `tf.Example` proto используя вышеприведенный словарь.\n",
    "  return tf.io.parse_example(example_proto, keys_to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОЧЕНЬ БЫСТРЫЙ ПОДСЧЕТ КОЛИЧЕСТВА СТРОК В ФАЙЛЕ\n",
    "from itertools import (takewhile,repeat)\n",
    "\n",
    "def rawincount(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    bufgen = takewhile(lambda x: x, (f.raw.read(1024*1024) for _ in repeat(None)))\n",
    "    return sum( buf.count(b'\\n') for buf in bufgen )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 164 ms, sys: 28 ms, total: 192 ms\n",
      "Wall time: 189 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rawincount(data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_start_positions_of_textfile(textfilepath, num_pos):\n",
    "    start_positions = []\n",
    "    \n",
    "    string_count = rawincount(textfilepath)\n",
    "    rows_in_one_part = string_count // num_pos\n",
    "#     print(\"Strings count:\", string_count)\n",
    "#     print(\"Rows in one part:\", rows_in_one_part)\n",
    "\n",
    "    opened_file = open(textfilepath, \"rt\")\n",
    "    for i in range(num_pos-1):\n",
    "        start_positions.append(opened_file.tell())\n",
    "        if len(start_positions) == num_pos:\n",
    "            break\n",
    "        else:\n",
    "            for j in range(rows_in_one_part):\n",
    "                opened_file.readline()\n",
    "    \n",
    "    return start_positions, rows_in_one_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_positions, rows_in_one_part = get_n_start_positions_of_textfile(textfilepath=data_filepath, num_pos=num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33333"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_in_one_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 4221058,\n",
       " 8427350,\n",
       " 12621918,\n",
       " 16808412,\n",
       " 20991602,\n",
       " 25182434,\n",
       " 29398134,\n",
       " 33568594,\n",
       " 37781198,\n",
       " 41974097,\n",
       " 46183838,\n",
       " 50394787,\n",
       " 54644410,\n",
       " 58836947,\n",
       " 63033386,\n",
       " 67238410,\n",
       " 71442983,\n",
       " 75621227,\n",
       " 79837691,\n",
       " 84042568,\n",
       " 88251386,\n",
       " 92472181,\n",
       " 96683396,\n",
       " 100871607,\n",
       " 105084280,\n",
       " 109280218,\n",
       " 113505763,\n",
       " 117713854]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the east cantons as a whole should therefore not be confused with the german language region created in nineteen sixty three or with the german speaking community of belgium which does not include the smaller malmedy and waimes areas\n",
      "'\n",
      "\n",
      "'we have also observed that many precalculus level students only employ mental action one and mental action two when asked to construct the graph of a dynamic function situation\n",
      "'\n",
      "\n",
      "'she had always loved playing classical violin which prompted her to leave the university after one year to attend a conservatory\n",
      "'\n",
      "\n",
      "'traffic clogged highways\n",
      "'\n",
      "\n",
      "'statement of right to rescind the sale length of trial period procedure for extending trial period procedure for rescinding sale and the date by which the hearing aid need to be returned to rescind the sale\n",
      "'\n",
      "\n",
      "'this institute is sponsored in part by the faculty development and instructional design centre and pepsi cola general bottlers incorporated\n",
      "'\n",
      "\n",
      "'the forty second amendment act of nineteen seventy six extended the initial time duration of state emergency from six months to one year\n",
      "'\n",
      "\n",
      "'rochester and beyond said president thomas h. jackson\n",
      "'\n",
      "\n",
      "'conservative m.p. brooks newmark asks why so little interest\n",
      "'\n",
      "\n",
      "'but chances are good that few have awoken hungover in a trashed caesars palace luxury suite among wandering chickens a crying baby in a closet and a very large tiger belonging to mike tyson\n",
      "'\n",
      "\n",
      "'deianira feared she would lose heracles to the younger and more beautiful iole\n",
      "'\n",
      "\n",
      "'bat and moth night eight thirty in the evening till dark finlaystone country estate langbank renfrewshire\n",
      "'\n",
      "\n",
      "'yancoal shares were trading at seventy four cents on tuesday afternoon up four cents since before the privatisation proposal was announced\n",
      "'\n",
      "\n",
      "'a major dock strike can halt the movement of goods\n",
      "'\n",
      "\n",
      "'wilkes's article\n",
      "'\n",
      "\n",
      "'in your world oprah winfrey's child needs more help than a non english speaking child of a illegal migrant waiter from russia or cambodia\n",
      "'\n",
      "\n",
      "'it is not possible to predict which cases will progress into s.c.c. so the current consensus is that all lesions should be treated\n",
      "'\n",
      "\n",
      "'i was surprised because she always looked like she was coping she said\n",
      "'\n",
      "\n",
      "'ferris state university serves off campus students and faculty through regional centres in northern southeast and southwest michigan and courses are delivered in cities throughout michigan\n",
      "'\n",
      "\n",
      "'he then became a high school principal in the state of washington when he was hired by onalaska high school onalaska washington where he also taught and coached\n",
      "'\n",
      "\n",
      "'subjective toxicities occurred in all health care workers and included nausea fatigue headache dyspepsia and or myalgia\n",
      "'\n",
      "\n",
      "'today not another first aeroplane\n",
      "'\n",
      "\n",
      "'action baglio will forward possible response based on s.g.b. e.c. decision to have sigchi move ahead with their election\n",
      "'\n",
      "\n",
      "'that is not the behaviour of a government that has nothing to hide kerry said adding that assad's forces had also destroyed evidence by shelling the area\n",
      "'\n",
      "\n",
      "'you might further argue that the stock market isn't perfectly efficient and prices don't immediately adjust to fair value leading to situations where sell stops might be beneficial\n",
      "'\n",
      "\n",
      "'but the president bush senate majority leader frist and senate judiciary committee chairman hatch also bear a significant amount of responsibility for failing to resolve or at least ameliorate the crisis\n",
      "'\n",
      "\n",
      "'the seismic data collected during this test represent the first instance of seismic while drilling with a p.d.c. bit and the first in an inclined well with a drill bit source\n",
      "'\n",
      "\n",
      "'his team worked on delta wing aeroplanes that were designed to accommodate a variety of new engines such as the turbojet and ramjet engines\n",
      "'\n",
      "\n",
      "'maigret keeps a photo of the late gallet with him throughout the investigation more as talisman than clue\n",
      "'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST START POSITIONS\n",
    "for pos in start_positions:\n",
    "    opened_file = open(data_filepath, \"rt\")\n",
    "    opened_file.seek(pos)\n",
    "    print(\"\\'\" + opened_file.readline() + \"\\'\\n\")\n",
    "    opened_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecord_gziped(args):\n",
    "    input_filepath, start_pos, lines_count, output_filepath = args\n",
    "    \n",
    "    opened_file = open(input_filepath, \"rt\")\n",
    "    opened_file.seek(start_pos)\n",
    "    \n",
    "    # Create output directory if absent\n",
    "    os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "    \n",
    "    options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "    writer = tf.io.TFRecordWriter(output_filepath + \".gzip\", options=options)\n",
    "    \n",
    "    for _ in range(lines_count):\n",
    "        line = opened_file.readline().strip()\n",
    "        encoded_line = np.array(bpe.encode(line), dtype=np.uint16)\n",
    "        if len(encoded_line) >= (SEQ_LEN + 1): # достаточно длинная строка\n",
    "            for i in range(len(encoded_line)-SEQ_LEN):\n",
    "                x, y = encoded_line[i:i+SEQ_LEN], encoded_line[i+SEQ_LEN: i+SEQ_LEN+1]\n",
    "                feature = {\n",
    "                    'X': _intlist_to_int64_feature(x),\n",
    "                    'y': _intlist_to_int64_feature(y)\n",
    "                }\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                writer.write(example.SerializeToString())\n",
    "    \n",
    "    writer.close()\n",
    "    opened_file.close()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecord(args):\n",
    "    input_filepath, start_pos, lines_count, output_filepath = args\n",
    "    \n",
    "    opened_file = open(input_filepath, \"rt\")\n",
    "    opened_file.seek(start_pos)\n",
    "    \n",
    "    # Create output directory if absent\n",
    "    os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "    \n",
    "    writer = tf.io.TFRecordWriter(output_filepath)\n",
    "    \n",
    "    for _ in range(lines_count):\n",
    "        line = opened_file.readline().strip()\n",
    "        encoded_line = np.array(bpe.encode(line), dtype=np.uint16)\n",
    "        if len(encoded_line) >= (SEQ_LEN + 1): # достаточно длинная строка\n",
    "            for i in range(len(encoded_line)-SEQ_LEN):\n",
    "                x, y = encoded_line[i:i+SEQ_LEN], encoded_line[i+SEQ_LEN: i+SEQ_LEN+1]\n",
    "                feature = {\n",
    "                    'X': _intlist_to_int64_feature(x),\n",
    "                    'y': _intlist_to_int64_feature(y)\n",
    "                }\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                writer.write(example.SerializeToString())\n",
    "    \n",
    "    writer.close()\n",
    "    opened_file.close()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 0, 788573, 'data_gziped_test/tfrecord_0_0')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 99475431, 788573, 'data_gziped_test/tfrecord_1_99475431')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 198923787, 788573, 'data_gziped_test/tfrecord_2_198923787')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 298424485, 788573, 'data_gziped_test/tfrecord_3_298424485')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 397758274, 788573, 'data_gziped_test/tfrecord_4_397758274')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 497190337, 788573, 'data_gziped_test/tfrecord_5_497190337')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 596532745, 788573, 'data_gziped_test/tfrecord_6_596532745')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 696051475, 788573, 'data_gziped_test/tfrecord_7_696051475')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 795485359, 788573, 'data_gziped_test/tfrecord_8_795485359')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 895074005, 788573, 'data_gziped_test/tfrecord_9_895074005')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 994572785, 788573, 'data_gziped_test/tfrecord_10_994572785')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 1093849771, 788573, 'data_gziped_test/tfrecord_11_1093849771')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 1193379224, 788573, 'data_gziped_test/tfrecord_12_1193379224')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 1292856488, 788573, 'data_gziped_test/tfrecord_13_1292856488')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 1392394788, 788573, 'data_gziped_test/tfrecord_14_1392394788')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 1491795688, 788573, 'data_gziped_test/tfrecord_15_1491795688')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 1591236080, 788573, 'data_gziped_test/tfrecord_16_1591236080')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 1690702976, 788573, 'data_gziped_test/tfrecord_17_1690702976')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 1790180559, 788573, 'data_gziped_test/tfrecord_18_1790180559')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 1889615141, 788573, 'data_gziped_test/tfrecord_19_1889615141')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 1989146056, 788573, 'data_gziped_test/tfrecord_20_1989146056')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 2088430085, 788573, 'data_gziped_test/tfrecord_21_2088430085')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 2187933094, 788573, 'data_gziped_test/tfrecord_22_2187933094')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 2287301564, 788573, 'data_gziped_test/tfrecord_23_2287301564')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 2386742427, 788573, 'data_gziped_test/tfrecord_24_2386742427')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 2486236256, 788573, 'data_gziped_test/tfrecord_25_2486236256')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 2585611737, 788573, 'data_gziped_test/tfrecord_26_2585611737')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 2684935113, 788573, 'data_gziped_test/tfrecord_27_2684935113')\n",
      "('/mnt/data5/docker/mini/share/spandh.ami1/data/rnnlm_data/test', 2784316638, 788573, 'data_gziped_test/tfrecord_28_2784316638')\n",
      "CPU times: user 9.27 s, sys: 1.34 s, total: 10.6 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "datapath = test_data_filepath\n",
    "start_positions, rows_in_one_part = get_n_start_positions_of_textfile(textfilepath=datapath, num_pos=num_cores)\n",
    "output_directory = \"data_gziped_test\"\n",
    "\n",
    "generator = (\n",
    "    (datapath, start_pos, rows_in_one_part, \"%s/tfrecord_%d_%d\" % (output_directory, idx, start_pos)) \n",
    "    for idx, start_pos in enumerate(start_positions)\n",
    ")\n",
    "\n",
    "# ### DEBUG\n",
    "# for i in generator:\n",
    "#     print(i)\n",
    "\n",
    "pool = multiprocessing.Pool(num_cores)\n",
    "pool.map(create_tfrecord_gziped, generator)\n",
    "\n",
    "sum_size = 0\n",
    "for path in glob.glob(\"%s/*\" % output_directory, recursive=True):\n",
    "    sum_size += os.stat(path).st_size\n",
    "print(sum_size / 1024. / 1024., \"Megabytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# datapath = test_data_filepath\n",
    "# start_positions, rows_in_one_part = get_n_start_positions_of_textfile(textfilepath=datapath, num_pos=num_cores)\n",
    "# output_directory = \"data_nonziped_test\"\n",
    "\n",
    "# generator = (\n",
    "#     (datapath, start_pos, rows_in_one_part, \"%s/tfrecord_%d_%d\" % (output_directory, idx, start_pos)) \n",
    "#     for idx, start_pos in enumerate(start_positions)\n",
    "# )\n",
    "\n",
    "# pool = multiprocessing.Pool(num_cores)\n",
    "# pool.map(create_tfrecord, generator)\n",
    "\n",
    "# sum_size = 0\n",
    "# for path in glob.glob(\"%s/*\" % output_directory, recursive=True):\n",
    "#     sum_size += os.stat(path).st_size\n",
    "# print(sum_size / 1024. / 1024., \"Megabytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.331963753987481"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# для входного 2845 мегабайтного файла\n",
    "26573.187690734863 / 6134.21284198761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.334855874906646"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# для входной 120 мегабайтного файла\n",
    "1124.935212135315 / 259.5092535018921"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### По времени, компрессированный сделался даже чуток быстрее 33м42с против 34м26с\n",
    "### По занимаемому месту, компрессированный весит в 4.33 раза меньше (6134.2 мегабайта против 26573.19 мегабайта)\n",
    "### По отношению к оригиналу компрессированный занимает в ~2.16 раза больше, некомпрес в 9.34 раза больше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ПРОВЕРИТЬ ПРАВИЛЬНО ЛИ СОЗДАЛИСЬ ФАЙЛЫ ДЛЯ ПОСЛЕДОВАТЕЛЬНОСТИ 10. ПОЧЕМУ ВЕС ПОЧТИ ТАКОЙЖЕ, ДАЖЕ ЧУТЬ МЕНЬШЕ !!!!!!!!!!!!!!!!!!!!!!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6972 {'X': <tf.Tensor: id=13991, shape=(13970, 10), dtype=int64, numpy=\n",
      "array([[  68,   12,  101, ...,   76,  865,  913],\n",
      "       [2598, 1315, 3786, ...,   54, 3508,  498],\n",
      "       [ 206, 1301,  245, ...,  104, 1557,   83],\n",
      "       ...,\n",
      "       [ 493, 3583,  152, ..., 2366,  331,  690],\n",
      "       [ 123,  112, 3420, ...,  417,   83, 1511],\n",
      "       [  61,   27,  154, ...,  154,  585,  351]])>, 'y': <tf.Tensor: id=13992, shape=(13970, 1), dtype=int64, numpy=\n",
      "array([[1445],\n",
      "       [ 109],\n",
      "       [ 932],\n",
      "       ...,\n",
      "       [ 193],\n",
      "       [ 694],\n",
      "       [1577]])>}\n",
      "CPU times: user 22min 35s, sys: 12min 40s, total: 35min 15s\n",
      "Wall time: 17min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Компрессированные\n",
    "input_directory = \"/mnt/data5/bocharick/tf2.0rc0_tst_prj/BPE_token_tests/data_gziped_test2\"\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(\n",
    "    filenames=glob.glob(\"%s/*\" % input_directory, recursive=True), \n",
    "    compression_type=\"GZIP\",\n",
    "    num_parallel_reads=num_cores\n",
    ").shuffle(buffer_size=200000)\n",
    "\n",
    "# SLOW\n",
    "# parsed_dataset = raw_dataset.map(_parse_function).batch(16000).prefetch(5)\n",
    "\n",
    "# FAST\n",
    "parsed_dataset = raw_dataset.batch(16000).map(_parse_batch_function).prefetch(5)\n",
    "\n",
    "for i, jj in enumerate(parsed_dataset):\n",
    "    pass\n",
    "print(i, jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6972 {'X': <tf.Tensor: id=28333, shape=(13970, 10), dtype=int64, numpy=\n",
      "array([[ 104,   50,  335, ..., 1309,  865, 1767],\n",
      "       [  66, 1804,   82, ...,  126,  609,  126],\n",
      "       [1891,   76,  923, ...,   80, 2757, 1812],\n",
      "       ...,\n",
      "       [1039, 2960, 1916, ..., 2710,  635,  203],\n",
      "       [1187,  465, 1744, ..., 2303,  102,  663],\n",
      "       [  54, 3744,  151, ...,  390,  819,   82]])>, 'y': <tf.Tensor: id=28334, shape=(13970, 1), dtype=int64, numpy=\n",
      "array([[ 76],\n",
      "       [363],\n",
      "       [941],\n",
      "       ...,\n",
      "       [995],\n",
      "       [278],\n",
      "       [ 54]])>}\n",
      "CPU times: user 22min 31s, sys: 12min 39s, total: 35min 10s\n",
      "Wall time: 17min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Некомпрессированные\n",
    "input_directory = \"/mnt/data5/bocharick/tf2.0rc0_tst_prj/BPE_token_tests/data_nonziped_test2\"\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(\n",
    "    filenames=glob.glob(\"%s/*\" % input_directory, recursive=True), \n",
    "    num_parallel_reads=num_cores\n",
    ").shuffle(buffer_size=200000)\n",
    "\n",
    "# SLOW\n",
    "# parsed_dataset = raw_dataset.map(_parse_function).batch(16000).prefetch(5)\n",
    "\n",
    "# FAST\n",
    "parsed_dataset = raw_dataset.batch(16000).map(_parse_batch_function).prefetch(5)\n",
    "\n",
    "for i, jj in enumerate(parsed_dataset):\n",
    "    pass\n",
    "print(i, jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0; time from start - 1.943 seconds; time from prev - 1.899 seconds\n",
      "i = 50; time from start - 10.166 seconds; time from prev - 8.221 seconds\n",
      "i = 100; time from start - 18.530 seconds; time from prev - 8.364 seconds\n",
      "i = 150; time from start - 26.955 seconds; time from prev - 8.422 seconds\n",
      "i = 200; time from start - 35.189 seconds; time from prev - 8.232 seconds\n",
      "i = 250; time from start - 43.398 seconds; time from prev - 8.208 seconds\n",
      "i = 300; time from start - 51.549 seconds; time from prev - 8.147 seconds\n",
      "i = 350; time from start - 59.747 seconds; time from prev - 8.197 seconds\n",
      "i = 400; time from start - 67.737 seconds; time from prev - 7.989 seconds\n",
      "i = 450; time from start - 75.837 seconds; time from prev - 8.099 seconds\n",
      "i = 500; time from start - 84.094 seconds; time from prev - 8.255 seconds\n",
      "i = 550; time from start - 91.957 seconds; time from prev - 7.862 seconds\n",
      "i = 600; time from start - 99.868 seconds; time from prev - 7.909 seconds\n",
      "i = 650; time from start - 107.963 seconds; time from prev - 8.094 seconds\n",
      "i = 700; time from start - 116.027 seconds; time from prev - 8.062 seconds\n",
      "i = 750; time from start - 124.015 seconds; time from prev - 7.987 seconds\n",
      "i = 800; time from start - 131.980 seconds; time from prev - 7.964 seconds\n",
      "i = 850; time from start - 140.123 seconds; time from prev - 8.142 seconds\n",
      "i = 900; time from start - 148.125 seconds; time from prev - 8.001 seconds\n",
      "i = 950; time from start - 155.932 seconds; time from prev - 7.806 seconds\n",
      "i = 1000; time from start - 163.927 seconds; time from prev - 7.994 seconds\n",
      "1000 {'X': <tf.Tensor: id=38570, shape=(16000, 10), dtype=int64, numpy=\n",
      "array([[  81,  442,  121, ...,   73,  959,   21],\n",
      "       [ 151, 3237,   20, ..., 1297, 1769, 1010],\n",
      "       [  58, 1844, 1669, ...,  169,  401, 2508],\n",
      "       ...,\n",
      "       [1707,   53,   83, ...,  330,  447,  284],\n",
      "       [ 335,  686,  720, ..., 3585, 1130,   61],\n",
      "       [  54, 1971,  152, ..., 3663,  697,   91]])>, 'y': <tf.Tensor: id=38571, shape=(16000, 1), dtype=int64, numpy=\n",
      "array([[ 435],\n",
      "       [2880],\n",
      "       [  83],\n",
      "       ...,\n",
      "       [ 726],\n",
      "       [ 802],\n",
      "       [3741]])>}\n",
      "CPU times: user 3min 13s, sys: 1min 57s, total: 5min 10s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Компрессированные\n",
    "st_time = time.time()\n",
    "\n",
    "input_directory = \"/mnt/data5/bocharick/tf2.0rc0_tst_prj/BPE_token_tests/data_gziped_test2\"\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(\n",
    "    filenames=glob.glob(\"%s/*\" % input_directory, recursive=True), \n",
    "    compression_type=\"GZIP\",\n",
    "    num_parallel_reads=num_cores\n",
    ").shuffle(buffer_size=200000)\n",
    "\n",
    "# SLOW\n",
    "# parsed_dataset = raw_dataset.map(_parse_function).batch(16000).prefetch(5)\n",
    "\n",
    "# FAST\n",
    "parsed_dataset = raw_dataset.batch(16000).map(_parse_batch_function).prefetch(5)\n",
    "\n",
    "prev_time = time.time()\n",
    "for i, jj in enumerate(parsed_dataset):\n",
    "#     pass\n",
    "    if i % 50 == 0:\n",
    "        print(\"i = %d; time from start - %.3f seconds; time from prev - %.3f seconds\" % (i, time.time() - st_time, time.time() - prev_time), flush=True)\n",
    "        prev_time = time.time()\n",
    "    if i == 1000:\n",
    "        break\n",
    "print(i, jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0; time from start - 2.462 seconds; time from prev - 2.421 seconds\n",
      "i = 50; time from start - 11.647 seconds; time from prev - 9.183 seconds\n",
      "i = 100; time from start - 19.937 seconds; time from prev - 8.287 seconds\n",
      "i = 150; time from start - 30.652 seconds; time from prev - 10.713 seconds\n",
      "i = 200; time from start - 40.803 seconds; time from prev - 10.148 seconds\n",
      "i = 250; time from start - 50.007 seconds; time from prev - 9.202 seconds\n",
      "i = 300; time from start - 58.699 seconds; time from prev - 8.688 seconds\n",
      "i = 350; time from start - 67.399 seconds; time from prev - 8.698 seconds\n",
      "i = 400; time from start - 76.400 seconds; time from prev - 8.999 seconds\n",
      "i = 450; time from start - 85.277 seconds; time from prev - 8.876 seconds\n",
      "i = 500; time from start - 94.432 seconds; time from prev - 9.153 seconds\n",
      "i = 550; time from start - 103.219 seconds; time from prev - 8.783 seconds\n",
      "i = 600; time from start - 112.284 seconds; time from prev - 9.061 seconds\n",
      "i = 650; time from start - 121.608 seconds; time from prev - 9.322 seconds\n",
      "i = 700; time from start - 130.801 seconds; time from prev - 9.191 seconds\n",
      "i = 750; time from start - 139.892 seconds; time from prev - 9.088 seconds\n",
      "i = 800; time from start - 149.123 seconds; time from prev - 9.229 seconds\n",
      "i = 850; time from start - 158.198 seconds; time from prev - 9.073 seconds\n",
      "i = 900; time from start - 167.840 seconds; time from prev - 9.641 seconds\n",
      "i = 950; time from start - 177.272 seconds; time from prev - 9.430 seconds\n",
      "i = 1000; time from start - 187.039 seconds; time from prev - 9.763 seconds\n",
      "1000 {'X': <tf.Tensor: id=32429, shape=(16000, 10), dtype=int64, numpy=\n",
      "array([[ 180,  111,   23, ..., 3258,  102,  126],\n",
      "       [  84,  364,  104, ...,  285,  509,  130],\n",
      "       [  26,   78, 1219, ...,   82, 1694,  547],\n",
      "       ...,\n",
      "       [1832,  160, 1513, ...,   54,  980,   96],\n",
      "       [  12,  212,   73, ...,  747,   54,   63],\n",
      "       [2750,  584, 1236, ...,  100,   13,   70]])>, 'y': <tf.Tensor: id=32430, shape=(16000, 1), dtype=int64, numpy=\n",
      "array([[ 724],\n",
      "       [  94],\n",
      "       [3183],\n",
      "       ...,\n",
      "       [1146],\n",
      "       [ 353],\n",
      "       [  70]])>}\n",
      "CPU times: user 3min 50s, sys: 2min 21s, total: 6min 12s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Компрессированные\n",
    "st_time = time.time()\n",
    "\n",
    "input_directory = \"/mnt/data5/bocharick/tf2.0rc0_tst_prj/BPE_token_tests/data_gziped_test\"\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(\n",
    "    filenames=glob.glob(\"%s/*\" % input_directory, recursive=True), \n",
    "    compression_type=\"GZIP\",\n",
    "    num_parallel_reads=num_cores\n",
    ").shuffle(buffer_size=200000)\n",
    "\n",
    "# SLOW\n",
    "# parsed_dataset = raw_dataset.map(_parse_function).batch(16000).prefetch(5)\n",
    "\n",
    "# FAST\n",
    "parsed_dataset = raw_dataset.batch(16000).map(_parse_batch_function).prefetch(5)\n",
    "\n",
    "prev_time = time.time()\n",
    "for i, jj in enumerate(parsed_dataset):\n",
    "#     pass\n",
    "    if i % 50 == 0:\n",
    "        print(\"i = %d; time from start - %.3f seconds; time from prev - %.3f seconds\" % (i, time.time() - st_time, time.time() - prev_time), flush=True)\n",
    "        prev_time = time.time()\n",
    "    if i == 1000:\n",
    "        break\n",
    "print(i, jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0; time from start - 2.438 seconds; time from prev - 2.402 seconds\n",
      "i = 50; time from start - 11.610 seconds; time from prev - 9.169 seconds\n",
      "i = 100; time from start - 22.509 seconds; time from prev - 10.898 seconds\n",
      "i = 150; time from start - 31.268 seconds; time from prev - 8.755 seconds\n",
      "i = 200; time from start - 40.083 seconds; time from prev - 8.813 seconds\n",
      "i = 250; time from start - 48.960 seconds; time from prev - 8.875 seconds\n",
      "i = 300; time from start - 57.825 seconds; time from prev - 8.860 seconds\n",
      "i = 350; time from start - 66.848 seconds; time from prev - 9.022 seconds\n",
      "i = 400; time from start - 76.172 seconds; time from prev - 9.320 seconds\n",
      "i = 450; time from start - 85.222 seconds; time from prev - 9.048 seconds\n",
      "i = 500; time from start - 95.095 seconds; time from prev - 9.870 seconds\n",
      "i = 550; time from start - 104.203 seconds; time from prev - 9.105 seconds\n",
      "i = 600; time from start - 113.450 seconds; time from prev - 9.245 seconds\n",
      "i = 650; time from start - 122.842 seconds; time from prev - 9.388 seconds\n",
      "i = 700; time from start - 131.962 seconds; time from prev - 9.119 seconds\n",
      "i = 750; time from start - 141.294 seconds; time from prev - 9.330 seconds\n",
      "i = 800; time from start - 150.618 seconds; time from prev - 9.320 seconds\n",
      "i = 850; time from start - 159.453 seconds; time from prev - 8.834 seconds\n",
      "i = 900; time from start - 168.366 seconds; time from prev - 8.911 seconds\n",
      "i = 950; time from start - 177.227 seconds; time from prev - 8.860 seconds\n",
      "i = 1000; time from start - 186.222 seconds; time from prev - 8.992 seconds\n",
      "1000 {'X': <tf.Tensor: id=34477, shape=(16000, 10), dtype=int64, numpy=\n",
      "array([[2137, 1016, 1244, ...,   83, 3049,  265],\n",
      "       [1403,  516, 1747, ...,  101,   50, 3687],\n",
      "       [ 465, 3904,  288, ...,   50, 2112,  163],\n",
      "       ...,\n",
      "       [ 200, 3482,   79, ...,  522,  635,   54],\n",
      "       [1721, 1015,   77, ...,   82, 2182, 1618],\n",
      "       [  10,  151,  491, ...,  104, 1290, 1848]])>, 'y': <tf.Tensor: id=34478, shape=(16000, 1), dtype=int64, numpy=\n",
      "array([[1889],\n",
      "       [  76],\n",
      "       [ 140],\n",
      "       ...,\n",
      "       [1725],\n",
      "       [ 770],\n",
      "       [ 871]])>}\n",
      "CPU times: user 3min 48s, sys: 2min 21s, total: 6min 9s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Компрессированные\n",
    "st_time = time.time()\n",
    "\n",
    "input_directory = \"/mnt/data5/bocharick/tf2.0rc0_tst_prj/BPE_token_tests/data_gziped_test\"\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(\n",
    "    filenames=glob.glob(\"%s/*\" % input_directory, recursive=True), \n",
    "    compression_type=\"GZIP\",\n",
    "    num_parallel_reads=num_cores\n",
    ").shuffle(buffer_size=200000)\n",
    "\n",
    "# SLOW\n",
    "# parsed_dataset = raw_dataset.map(_parse_function).batch(16000).prefetch(5)\n",
    "\n",
    "# FAST\n",
    "parsed_dataset = raw_dataset.batch(16000).map(_parse_batch_function).prefetch(num_cores)\n",
    "\n",
    "prev_time = time.time()\n",
    "for i, jj in enumerate(parsed_dataset):\n",
    "#     pass\n",
    "    if i % 50 == 0:\n",
    "        print(\"i = %d; time from start - %.3f seconds; time from prev - %.3f seconds\" % (i, time.time() - st_time, time.time() - prev_time), flush=True)\n",
    "        prev_time = time.time()\n",
    "    if i == 1000:\n",
    "        break\n",
    "print(i, jj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0; time from start - 2.859 seconds; time from prev - 2.729 seconds\n",
      "i = 50; time from start - 60.463 seconds; time from prev - 57.603 seconds\n",
      "i = 100; time from start - 109.824 seconds; time from prev - 49.358 seconds\n",
      "i = 150; time from start - 160.080 seconds; time from prev - 50.250 seconds\n",
      "i = 200; time from start - 208.137 seconds; time from prev - 48.056 seconds\n",
      "i = 250; time from start - 257.311 seconds; time from prev - 49.173 seconds\n",
      "i = 300; time from start - 312.425 seconds; time from prev - 55.112 seconds\n",
      "i = 350; time from start - 359.356 seconds; time from prev - 46.930 seconds\n",
      "i = 400; time from start - 406.856 seconds; time from prev - 47.498 seconds\n",
      "i = 450; time from start - 454.884 seconds; time from prev - 48.027 seconds\n",
      "i = 500; time from start - 505.873 seconds; time from prev - 50.987 seconds\n",
      "i = 550; time from start - 554.900 seconds; time from prev - 49.025 seconds\n",
      "i = 600; time from start - 607.513 seconds; time from prev - 52.611 seconds\n",
      "i = 650; time from start - 656.199 seconds; time from prev - 48.685 seconds\n",
      "i = 700; time from start - 710.760 seconds; time from prev - 54.559 seconds\n",
      "i = 750; time from start - 758.092 seconds; time from prev - 47.330 seconds\n",
      "i = 800; time from start - 806.933 seconds; time from prev - 48.840 seconds\n",
      "i = 850; time from start - 857.278 seconds; time from prev - 50.344 seconds\n",
      "i = 900; time from start - 906.365 seconds; time from prev - 49.085 seconds\n",
      "i = 950; time from start - 956.381 seconds; time from prev - 50.014 seconds\n",
      "i = 1000; time from start - 1006.317 seconds; time from prev - 49.935 seconds\n",
      "1000 {'X': <tf.Tensor: id=36522, shape=(16000, 10), dtype=int64, numpy=\n",
      "array([[ 402,  380,  233, ..., 3756, 1653,   83],\n",
      "       [ 234, 1574,  609, ...,   54, 1492, 2918],\n",
      "       [ 577,  117,   74, ...,  250,   19, 2902],\n",
      "       ...,\n",
      "       [ 819,  111,  192, ...,   65, 1457, 1303],\n",
      "       [ 145,  278,  166, ...,   26,  623, 1874],\n",
      "       [ 166, 3611, 1812, ..., 3449,  147,   73]])>, 'y': <tf.Tensor: id=36523, shape=(16000, 1), dtype=int64, numpy=\n",
      "array([[361],\n",
      "       [281],\n",
      "       [ 79],\n",
      "       ...,\n",
      "       [ 82],\n",
      "       [ 76],\n",
      "       [ 72]])>}\n",
      "CPU times: user 23min 41s, sys: 6min 43s, total: 30min 24s\n",
      "Wall time: 16min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Компрессированные\n",
    "st_time = time.time()\n",
    "\n",
    "input_directory = \"/mnt/data5/bocharick/tf2.0rc0_tst_prj/BPE_token_tests/data_gziped_test2\"\n",
    "\n",
    "raw_dataset = tf.data.TFRecordDataset(\n",
    "    filenames=glob.glob(\"%s/*\" % input_directory, recursive=True), \n",
    "    compression_type=\"GZIP\",\n",
    "    num_parallel_reads=num_cores\n",
    ").shuffle(buffer_size=200000)\n",
    "\n",
    "# SLOW\n",
    "parsed_dataset = raw_dataset.map(_parse_function).batch(16000).prefetch(5)\n",
    "\n",
    "# FAST\n",
    "# parsed_dataset = raw_dataset.batch(16000).map(_parse_batch_function).prefetch(5)\n",
    "\n",
    "prev_time = time.time()\n",
    "for i, jj in enumerate(parsed_dataset):\n",
    "#     pass\n",
    "    if i % 50 == 0:\n",
    "        print(\"i = %d; time from start - %.3f seconds; time from prev - %.3f seconds\" % (i, time.time() - st_time, time.time() - prev_time), flush=True)\n",
    "        prev_time = time.time()\n",
    "    if i == 1000:\n",
    "        break\n",
    "print(i, jj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение файлов:  \n",
    "Из датасета читалось 1000 батчей, каждый размером 16000 записей, с шафлингом датасета .shuffle(buffer_size=200000).  \n",
    "В итоге обнаружил, что, оказывается, можно еще и \"неправильно\" читать данные в плане скорости.  \n",
    "Конкретно в данном случае существенная разница в скорости была из за того что сначала в медленном, как выяснилось, варианте, бралась запись из файла, затем десериализовалась, и затем набирался батч. А если же взять недесериализованные данные, собрать из них батч, а потом уже весь батч десериализовать, получается значительная прибавка в производительности.  \n",
    "  \n",
    "Чтение медленным способом (парсинг каждой записи):  \n",
    "#### 1) Чтение из 7 gzip файлов в num_parallel_reads=30, prefetch(5) за 16м47с\n",
    "\n",
    "Чтение быстрым способом (парсинг батчей):  \n",
    "#### 1) Чтение из 7 gzip файлов в num_parallel_reads=30, prefetch(5) за 2м44с  \n",
    "#### 2) Чтение из 30 gzip файлов в num_parallel_reads=30, prefetch(5) за 3м7с  \n",
    "#### 3) Чтение из 30 gzip файлов в num_parallel_reads=30, prefetch(30) за 3м6с  \n",
    "\n",
    "То есть размер prefetch не влияет в данном случае на скорость чтения, а бОльшее количество файлов с параллельным чтением, видимо увеличило нагрузку на память, так как параллельно идет распаковка на лету всеми файлами. По идее имеет смысл брать компромиссное количество, а брать совсем максимальное во все ядра может и не требуется. Суть просто в том, что когда у тебя N параллельных файлов, то в одном батче набираются строки записи из каждого из этих файлов. То есть, если у тебя 30 файлов, а в батче 60 записей, то в итоге в батче будет по 2 строки из каждого из 30 файлов. Своего рода шаффлинг обеспечиваемый загрузкой, помимо еще шаффла который можно принудительно указать для tf.data.Dataset.  \n",
    "\n",
    "Итого получается, что чтение в одной и той же ситуации, но с разницей в сериализации построчно или побатчево отличается в ~6.14 раз по скорости.  \n",
    "\n",
    "Это все, конечно, по факту знания уже становится очевидно, но изначально, когда ковырялся в доках датасета, там десериализацию показывали именно построчно, и как-то не этот факт вообще внимания не обращал, до тех пор пока просто не показалось, что чтение достаточно медленное.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
